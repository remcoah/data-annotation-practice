# LLM Evaluator Bootcamp ğŸ§ ğŸ’»

This is a personal coding practice repository to help me relearn Python, sharpen my logic, and prepare for jobs in AI/data-related roles â€” starting with Data Annotation & LLM Output Evaluation.

## ğŸ¯ Goals

- Rebuild Python fundamentals
- Practice writing and reviewing Python code
- Evaluate outputs from Large Language Models (LLMs)
- Understand and apply concepts like precision vs accuracy
- Prepare for real-world technical interviews and assessments

## ğŸ› ï¸ What's Inside

- `even_odd.py` â€“ Simple logic function to determine if a number is even or odd
- `palindrome.py` â€“ Function to check if a word is a palindrome
- `llm_evaluations/` â€“ Side-by-side comparisons of mock LLM outputs with explanations
- `notes.md` â€“ Quick notes, tips, and LLM bug patterns

## ğŸ“š Technologies Used

- Python 3.x
- Git + GitHub
- Visual Studio Code (VS Code)

## ğŸ’¡ Why This Repo Exists

I'm currently preparing for a Data Annotation role where I compare LLM-generated code outputs based on their precision and accuracy. This repo tracks my progress as I level up my coding skills and get ready for real-world tasks and interviews.

## ğŸ“ˆ Progress

- âœ… Relearned basic Python syntax and logic
- âœ… Completed 3 mock LLM code evaluations
- â³ Building fluency with Git and VS Code
- â³ Connecting repo to GitHub & pushing updates
- â³ Creating more advanced examples & mini projects

## ğŸ“ Next Steps

- Add more practice problems (functions, loops, lists, strings)
- Simulate full mock tests
- Practice writing feedback like a real data annotator
- Build a mini tool to auto-rank code outputs

---

### ğŸ“¬ Questions or feedback?
This is a solo bootcamp, but feel free to fork or use it for your own practice!

