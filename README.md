# LLM Evaluator Bootcamp 🧠💻

This is a personal coding practice repository to help me relearn Python, sharpen my logic, and prepare for jobs in AI/data-related roles — starting with Data Annotation & LLM Output Evaluation.

## 🎯 Goals

- Rebuild Python fundamentals
- Practice writing and reviewing Python code
- Evaluate outputs from Large Language Models (LLMs)
- Understand and apply concepts like precision vs accuracy
- Prepare for real-world technical interviews and assessments

## 🛠️ What's Inside

- `even_odd.py` – Simple logic function to determine if a number is even or odd
- `palindrome.py` – Function to check if a word is a palindrome
- `llm_evaluations/` – Side-by-side comparisons of mock LLM outputs with explanations
- `notes.md` – Quick notes, tips, and LLM bug patterns

## 📚 Technologies Used

- Python 3.x
- Git + GitHub
- Visual Studio Code (VS Code)

## 💡 Why This Repo Exists

I'm currently preparing for a Data Annotation role where I compare LLM-generated code outputs based on their precision and accuracy. This repo tracks my progress as I level up my coding skills and get ready for real-world tasks and interviews.

## 📈 Progress

- ✅ Relearned basic Python syntax and logic
- ✅ Completed 3 mock LLM code evaluations
- ⏳ Building fluency with Git and VS Code
- ⏳ Connecting repo to GitHub & pushing updates
- ⏳ Creating more advanced examples & mini projects

## 📍 Next Steps

- Add more practice problems (functions, loops, lists, strings)
- Simulate full mock tests
- Practice writing feedback like a real data annotator
- Build a mini tool to auto-rank code outputs

---

### 📬 Questions or feedback?
This is a solo bootcamp, but feel free to fork or use it for your own practice!

